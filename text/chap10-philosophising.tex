\begin{savequote}[8cm]

	‘The supreme maxim in scientific philosophizing is this: wherever possible, logical constructions are to be substituted for inferred entities.’
	
	\qauthor{--- Bertrand Russell, \textit{\usebibentry{russell1914relation}{title}} \citeyearpar{russell1914relation} }
	
\end{savequote}

\chapter{\label{ch:10-philosophising}A little bit of scientific philosophising}
%\otherpagedecoration


\minitoc{}

% https://archive.org/details/problemsofphilo00russuoft/page/n11

\begin{quote}
	\textit{‘Is there any knowledge in the world which is so certain that no reasonable man could doubt it?}
\end{quote}

This question, which was the first sentence of the book \textit{\usebibentry{russell1912problems}{title}} \citeyearpar{russell1912problems} by Bertrand Russell (1872--1970), summarises rather well his life's quest: the search for truth — which he believed could be attained with logic.
Russell spent all his life working on this topic, both in mathematics and in philosophy, and this made him one of the founding fathers of contemporary logic.
We can get a small taste of his logical developments in the paradox he discovered in the domain of set theory, and which he himself translated into ‘ordinary language’ \citep{russell1918philosophy} under the form of the barber's paradox:

\begin{quote}
	\textit{‘You can define the barber as “one who shaves all those, and those only, who do not shave themselves”. The question is, does the barber shave himself?’}
\end{quote}

Answering this question results in a contradiction: if he shaves himself, he cannot shave himself (because the barber shaves only those who do not shave themselves); and if he does not shave himself, he must shave himself (because the barber shaves all those who do not shave themselves). This is a typical ‘logical paradox’.

For Russell, the solution to such contradictory phenomena is to break down each proposition (scientific or philosophic) into ultimate logical units (or atoms) which can be understood independently of other units: this is what he called ‘logical atomism’.
In his view, to know whether a proposition is true or false comes back to analysing the veracity of each atom and the relationship between them\footnote{This is, by the way, what led him to redemonstrate every simplistic principle of algebra (like the fact that $1+1=2$) in his \textit{\usebibentry{whitehead1912principia}{title}} \citeyearpar{whitehead1912principia}.}.
To further decide on the veracity of a given simple proposition, — which he redefines as the adequacy between a belief and a fact, — 
one must agree to hierarchise the degree of certainty of each ‘known’ fact.
For instance, one can be absolutely certain of the things they directly experimented with their five senses (‘sense-data’), — he calls that ‘knowledge by acquaintance’, — but the confidence one has in ‘knowledge by induction’ (i.e.\ the process of deriving a theory from the repeated observation of events) must be questioned.
To borrow one of his own illustrations of that matter \citep{russell1912problems}, we do not feel the slightest doubt that the sun will rise tomorrow because of the laws of motion.
\textit{‘But the \textit{only} reason for believing that the laws of motion will remain in operation is that they have operated hitherto, so far as our knowledge of the past enables us to judge. […] But the real question is: do any number of cases of a law being fulfilled in the past afford evidence that it will be fulfilled in the future?’}
It is, of course, highly unlikely that the laws of motion would stop tomorrow and that the sun would not rise; though, we cannot \textit{prove} it is impossible and, thus, the degree of confidence we can have in such knowledge is lower than that for the things we are directly acquainted with, like the fact that the paper on which this text is written is white.


In the case of an evolutionary force, we are in a typical case of such knowledge learnt by induction: 
we infer its very existence on the basis of the observation of its consequences on genomes and, on top of that, it is not even tangible, but merely a concept useful to theorise how genomes evolve.
To analyse such ideas, Russell systematically started with redefining precisely the terms.
But, what is an evolutionary force, exactly? Is it even a cause (for genome evolution) or a consequence (of the molecular processes taking place in individuals)? And, consequently, at what scale — individual or populational — should it be studied?
In the first section of this chapter, I will try and provide ideas to answer those questions.
In the second section, I will dive into the more general notion of the way scientific knowledge can be obtained and the context in which it arises and last, I will focus on the particular and more recent role of bioinformatics in acquiring such knowledge.





\section{About evolutionary forces}

\subsection{Forces as conceptual frameworks} 

By definition, a force represents an interaction which, if unopposed, can change the motion of an object.
As such, forces are generally viewed as causes driving objects or phenomena in a certain direction and are commonly symbolised as vectors giving their direction and intensity.
But are forces mere conceptual tools useful to better apprehend physical phenomena, or could they exist as real physical entities?

Gravitation, which ensures the mechanical movement of planets and other celestial bodies, is a most interesting case study to think of the aforementioned interrogation.
Indeed, for over 200 years, the theory formulated by Isaac Newton (1642--1727) — the law of universal attraction stating that a ‘gravitational force’ leads masses to attract one another — had been widely accepted.
But, in the early 1900's, Albert Einstein (1879--1955) established the theory of general relativity which accounted for the physical effects unexplained by Newton's law and contradicted the idea that the gravitational force was even a force at all: instead, gravitational attraction would be the result of the warping of spacetime by large masses.
Since then, gravitation has stopped being considered as a force, but its pictural representation as vectors has nonetheless persited, for it helps conceptualising the physical phenomena it explains.




\subsection{Forces as emerging properties of individuals}

Another way to regard forces consists in perceiving them as emerging properties of the individuals (particles, people, cells, etc…) which constitute them, i.e.\ as phenomena resulting from the intrinsic characteristics of their components, but \textit{not} reductible to the latter. 
In other words, a force would be the consequence of the fundamental properties of its components, but somehow more than the mere sum of its parts.

To borrow once again an example taken from physics, pressure corresponds to the mean action of the collision of gas particles on a given area and, thus, arises from the intrinsic properties of its components.
Though, each of these particles moves completely randomly (‘Brownian motion’) and does not cease bumping into other molecules or into the surfaces of the walls. 
As such, pressure \textit{cannot} be seen in any particle by itself (for its trajectory is random and the force it exerts on an area is unpredictable) but it nonetheless \textit{emerges} from the collective action of many.

In a totally different context, what is called peer pressure results from the individual choices of single people and can thus be seen as a consequence of the biological processes occurring inside their brains.
When looked at it at the scale of a population though, this phenomenon becomes the root cause of the behaviour, attitude or values of other people to conform to the influencing group.
As such, peer pressure — and the same would apply to other sociological phenomena, like consumer behaviour — can be seen both as a cause or as a consequence, depending on the point of view.\\

Altogether thus, even if forces are most generally used as concepts useful to understand phenomena (whether physical, biological, sociological, or else), they are the result of more fundamental properties emerging from their individual components.
With this in mind, at what scale, — populational or individual, — would it be most meaningful to study them in the context of evolutionary biology?





\subsection{Processes \textit{versus} patterns} 

In the 1930's and 1940's, the modern synthesis (a.k.a.\ neo-Darwinian synthesis), — which was formally defined by \citet{dobzhansky1937genetic}, \citet{huxley1942evolution}, \citet{mayr1942systematics} and \citet{simpson1944tempo} — reconciled Darwin's theory of evolution and Mendel's ideas on heredity (see Chapter~\ref{ch:1-history-genetics}).

Since then, the way of considering the objects of study in evolution and their relationships has considerably changed \citep[reviewed in][]{paulin2015epistemologie}: a bipolarisation between \textit{patterns} (i.e.\ the description of the results of evolution, as independently as possible from any explanatory theory) and \textit{processes} (i.e.\ the mechanisms responsible for evolution) emerged.
What is less well known is that this distinction was defensibly already present in Darwin's theory \citep{gayon2018connaissance} as the name he gave it — ‘descent with modification by means of natural selection’ — suggests: the ‘descent with modification’ part would correspond to the \textit{patterns} of evolution and the ‘by means of natural selection’ part to the \textit{processes} leading to it.

This distinction could arguably be applied to the study of evolutionary forces as well.
In the case of the object of this thesis, — biased gene conversion (BGC), — the \textit{process} would correspond to the functional study of the way the molecular machinery responsible for the repair of DNA mismatches results in BGC, and the \textit{pattern} to describing its deleterious consequences on genomes and the extent to which it induces divergence between them.
As such, the joint study of both aspects seems essential to describe this evolutionary force as a whole.

Though, the distinction between \textit{patterns} and \textit{processes} may be too simplistic, and it has been much criticised by Stephen Jay Gould (1946--2002) and Niles Eldredge (born 1943) from the 1970's on \citep[reviewed in][]{dericqles2009quelques}.
Their major objection concerned gradualism (i.e.\ the idea that all evolutionary changes are slow, gradual and cumulative) because this implied that there would be a nearly total determinism of micro-evolution (\textit{processes}) onto macro-evolution (\textit{patterns}) and that almost everything could be explained by the sole action of natural selection and adaptation \citep[reviewed in][]{paulin2015epistemologie}.
Instead, Gould put into perspective the extent to which such deterministic features contributed to macro-evolution by reintroducing historical contingency, i.e.\ the idea that the history of life also depends on a series of historical events that are often random or, at least, unpredictable \citep{gould1989wonderful}.

As such, even though his view is still debated, Gould managed to question parts of a theory which was already widely accepted by the scientific community.
The way through which such novel ideas can spread into the scientific world participates much in the progress of science and represents one of the main questions tackled by epistemologists. 
As such, I will focus on this issue in the following section.







\section{About scientific advances}
\subsection{Scientific revolutions and paradigm shifts} 


To face gradualism in the modern synthesis of evolution, Gould and Eldredge put forward another thesis: the theory of punctuated equilibria, according to which periods of rapid change are followed by longer periods of relative stasis, i.e.\ states of little change \citep{gould1972punctuated}.\\

We could draw a parallel between this new theory about evolution and that by Thomas Samuel Kuhn (1922--1996) about scientific progress.
Indeed, when it began in the eighteenth century, history of science was written by scientists who presented the discoveries of their time as the culmination of a long process of advancing knowledge.
Thus, science was perceived as a progressive accumulation of cognition where true theories replaced false beliefs \citep{golinski2008making}.

In contrast, Kuhn portrayed scientific progress as a cyclic process involving paradigm shifts, i.e.\ fundamental changes in the basic principles of a scientific discipline \citep{kuhn1962structure}.
In his view, periods of ‘normal science’ where scientists work under a conceptual framework which works globally well alternate with shorter periods of ‘revolutionary science’ where the repeated detection of anomalies (i.e.\ observations unreconciliable with the paradigm of the time) leads to another paradigm under which the world that scientists perceive, as well as the principles, methods or even language they use, are different.\\

According to Kuhn, the transition from one paradigm to another does not rest solely on rational scientific reasons justifying that the new paradigm would be more accurate: he firmly believes that these major shifts also largely depend on external factors, like the sociological and ideological context of the time. I give examples of these in the following subsection.








\subsection{The impact of external factors} 



Paul Forman (born 1937), a former student of Kuhn's, defended the thesis of a cultural conditioning of scientific knowledge.
He developed his proposition with the example of the connection between the culture of Weimar Germany and the emergence of quantum mechanics in the 1920's \citep{forman1971weimar}.
According to him, in the aftermath of the defeat of Germany in World War I, the dominant tendancy was characterised by intellectual revolts against causality, determinism and materialism and welcomed the rise of anti-rationalist movements such as existentialism, i.e.\ a philosophy of life claiming that individuals are faced with the absurdity of life and that the essence of their being lies in their own actions which are \textit{not} predetermined by any kind of theological, philosophical or moral doctrine.

In Forman's view, the concept of quantum \textit{a}causality could spread much more easily into this German scientific world marked by the rejection of determinism and analytical rationality than in other Western countries which did not undergo such crises, and explains why the most prominent advances in that field were made by Germans.\\

On top of the sociological, political and religious context, Barry Barnes (born 1943) argues that the personal interests of researchers also play a major role in determining their actions and, thus, in shaping scientific advances \citep{barnes1977interests}.
Interests at stake in scientific practice may include the use of techniques or theories specific to a given paradigm which they want to promote, or defined by their social, political or ideological position \citep{gingras2017determinants}.
As such, ‘inner’ and ‘outer’ factors are not necessarily distinct.

For instance, in nowadays world where ecological awareness is growing, several scientists promote the creation of a new geological epoch — the so-called ‘Anthropocene’ — that would account for the impact of mankind on Earth's geology and ecosystems \citep{crutzen2002geology} and some geologists and mineralogists have already started doing research in this still unofficial field of investigation \citep{corcoran2014anthropogenic,hazen2017mineralogy}.\\



In the case of Gould and Eldredge too, their challenging the modern synthesis was made possible thanks to the contemporary creation of additional fields of investigation — including developmental genetics, phylogenetic cladistics, the molecular clock and gene transfers: these provided novel findings or original ways of thinking, which participated a great deal in questioning parts of the modern synthesis \citep{lecointre2009apres}.

Generally, the creation of new domains of study pairs up with the establishment of modern techniques which themselves play a significant role in advancing knowledge. 
I discuss this topic in the next subsection. 




\subsection{The contribution of modern techniques}

It goes without saying that scientific knowledge has systematically considerably benefited from both technological advances and the expertise of scientists in using the latter.
Cell biology, for one, would not have existed had microscopy not been invented \citep{bechtel2006discovering} and chromosomes would not have been discovered if it had not been for Frans Janssens's mastery of cell staining (see Chapter~\ref{ch:1-history-genetics}).

Though, the very use of technologies for scientific progress can bring a set of questions of its own.
Indeed, it has been argued that there is often a circular relationship between the pieces of evidence for a phenomenon of interest and the instruments detecting it \citep[reviewed in \citealp{godin2002experimenters}]{collins1975seven,collins1985changing}: 
according to the words of the sociologist who developed this idea, ‘\textit{we won't know if we have built a good detector until we have tried it and obtained the correct outcome. But we don't know what the correct outcome is until… and so on ad infinitum}’ \citep{collins1985changing}.
He termed this pitfall the ‘experimenter's regress’.

On top of that, the belief (or not) in the outcome and the acceptance (or not) of the value given by the instrument somehow depends on the researcher's interests: a scientist who believes in the existence of a phenomenon will be willing to accept the announcement of its detection, while one who does not would probably rather question the validity of either the apparatus or the method used \citep{gingras2017determinants}.\\


In genetics, the development of the first sequencing techniques in the 1970's have led to a major upheaval in the way research is carried.
Indeed, the rise of ‘-omics’ (genomics, transcriptomics, metabolomics, proteomics, etc…) as major fields of study, together with the large progresses in computing resources and data storage capacity, has led some to re-think of the interplay between data-driven and hypothesis-driven science \citep{kell2004here, mazzocchi2015could}.

But, from now on, future advances in the field surely depend much more on the ability of bioinformaticians to analyse the deluge of data standing before them rather than on further technological leaps.
In the last section, I thus share my vision on the way I believe bioinformaticians can best help scientific progress.


\section{About bioinformaticians}

\subsection{Biologists before informaticians} 

The word ‘bioinformatics’ is a contraction of ‘biology’ and ‘informatics’ and both facets are of course required in this domain.
Though, it seems to me that, in view of the colossal quantity of data that genomicians are supposed to deal with, it can be tempting to let the informatics side take over.
On top of that, some bioinformaticians perceive results obtained purely by an automated process involving bioinformatic tools with little or no input from the experimenter as objective, and negatively regard as subjective any choice made by the biologist.

I would like to argue against that line of reasoning by taking an example from machine learning — a set of methods which has begun to be used by bioinformaticians in the last few years.
Basically, machine learning is a subset of artificial intelligence aiming at ‘learning’ from data.
In the vast majority of cases, these programs ‘learn’ on the basis of the correlations they find within the training sets they are provided with.
Retracing how these associations have been made is actually a rather complex process but, in one study, after creating a classifier allowing to distinguish between dogs and wolves, \citet{ribeiro2016why} wanted to understand the reasons why their artificial-intelligence method was so outstandingly accurate.
They analysed the associations made by the program and found out that the main feature used to distinguish between the two animals was the background in the training pictures: wolves were often standing on snow whereas dogs were rather standing on grass.
As such, even if the classifier outputted the correct results, it became obvious that it could not be trusted.
Nevertheless, such caveats originating from automated processes can easily be avoided by human knowledge.

In the context of this thesis, the method we implemented to detect recombination events from sequencing data rested on identifying and iteratively suppressing sources of error (see Chapters~\ref{ch:5-methodology} and~\ref{ch:8-HFM1}).
In the process leading to it, a considerable amount of time was spent visually inspecting the candidate events and hypothesising on the origin of miscalls.
Automation was only used in a second phase to assess the impact of each possible adjustment to the final outcome.
This is, by the way, together with the crucial role of negative controls, how we could identify that mapping biases explained most of the false positive miscalls.

As such, I firmly believe that the input from any savvy human can make analyses much more accurate than the sole work of bioinformatic tools.\\

I would also like to argue in favour of simplicity.
Indeed, considering the extremely wide range of bioinformatic tools — but also statistical and mathematical methods — available today, it is often tempting to create sophisticated processes to tackle biological problems that are generally rather complex.
Though, it seems to me that, except for some specific issues, aiming at the maximal simplicity carries many advantages, including a better reproducibility of analyses, a more straightforward detection of errors, greater smooth in adapting code or methods to other frameworks and much larger clarity in transmitting the ideas.





\subsection{Training biologists in genomics}

With the ever increasing amount of sequencing data available, one of the major limitations in genomics becomes the ability to process them.
I argued in the previous subsection that the input from humans — biologists in the case of bioinformatics — was crucial to analyse the data correctly.

Though, it is not that easy for biologists to get trained in bioinformatics: to the extent of my knowledge, there is no free website that explains the basic know-how of next-generation sequencing data analysis.
Therefore, I decided to create one (\url{https://gnomics.io/}) to account for this lack.
In it, I try to provide biologists with a global overview of the major steps that one should follow to perform the most common genomic analyses, indicate the tools allowing to complete each of these and the way to use them concretely and, finally, explain the assumptions on which they are based and the way the outcome they render should be interpreted.




\subsection{A genomician in evolutionary biology}

According to the paleontologist Stephen Jay Gould, evolutionary biology is a kind of science somewhat special in the way that it creates knowledge.
Indeed, in most research fields, the best way to know whether a hypothesis is true or false consists in experimentally testing for it and comparing the outcome it predicted to the real one: if they concord, the hypothesis may be true; otherwise, we can be sure that it is false.
Though, this so-called scientific method is not adapted to the study of evolution because the objects of study cannot be reproduced experimentally\footnote{Nevertheless, this is precisely what studies of so-called ‘experimental evolution’ aim to do.}.
Instead, the past is to be \textit{inferred} and, arguably, if there was a past, remnants of it should persist in today's world.
The whole work of the evolutionary biologist thus consists in searching for these relics — which, according to Gould, are often imperfections or incongruities — and to make sense of them in a more global picture of evolution \citep{gould1979turtles}.

In this context, a genomician working in evolutionary biology should scan genomes to try and find vestiges of the past which could help reconstruct indirectly the unobservable evolutionary history.
The discovery of biased gene conversion was typically such a case of evolutionary inference based on unexplained incongruities seen in genomes:
it all started with the strange observation that GC-content varies along genomes (see Chapter~\ref{ch:4-gBGC}).
Several hypotheses were then proposed to explain it — one of which being the existence of biased gene conversion.
Since then, a lot of work — including that carried for this thesis, — has been done with the aim of providing evidence for this hypothesis.

Bioinformaticians generally have a training in either informatics, algorithmics, mathematics, statistics or any other field in which certainty is much more widespread than in biology, and especially more than in evolutionary biology.
As such, for them to work in this research field, I would argue that one of the major difficulties may reside in fighting an inner struggle to make room for doubt in the middle of all the apparent objectivity of computer programs.\\







All in all, science is not much different than a quest for truth and scientists generally try and pursue objectivity so as to get to it.
Though, in this chapter where I gathered epistemological, philosophical and sociological thoughts, I showed that scientific progress also depends on the contingency of external events and on the subjective interests of researchers, no matter how neutral they are willing to be.
In the particular case of bioinformatics applied to evolutionary genomics, I believe that the subjectivity of human expertise can be used as an advantage rather than as an obstacle to make further progress.
It was with these thoughts in mind that the work useful to this thesis was carried.
As for now, there is nothing left for me but to conclude about it all.




